# -*- coding: utf-8 -*-
"""adahal5_#2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/111c5CS5iayaXqKM-1OCGxzPUB1YZVDKc

# **Convolutional Neural Network for Cats vs.Dogs**

# Mount Google Drive to access the dataset
"""

from google.colab import drive
    drive.mount('/content/drive')

"""Connects to Google Drive for dataset access.

#Importing necessary libraries
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import VGG16
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
import shutil
import time
from datetime import datetime

"""Uses TensorFlow/Keras for deep learning, NumPy for numerical operations, Matplotlib for visualization, etc.

# Set seed for reproducibility
"""

np.random.seed(42)
tf.random.set_seed(42)

"""Ensures reproducibility of results.

# Define dataset paths
"""

import pathlib
base_dir = pathlib.Path ('/content/drive/MyDrive/cats_vs_dogs_small (1)')
train_dir = base_dir / 'train'
validation_dir = base_dir / 'validation'
test_dir = base_dir / 'test'

"""Points to training, validation, and test directories.

# Check if directories exist
"""

if not train_dir.exists() or not validation_dir.exists() or not test_dir.exists():
    raise FileNotFoundError("Dataset directories not found. Please check the dataset path.")

print(f"Train Directory: {train_dir}")
print(f"Validation Directory: {validation_dir}")
print(f"Test Directory: {test_dir}")

"""# Data augmentation and preprocessing"""

train_datagen = ImageDataGenerator(
rescale=1./255,
rotation_range=40,
width_shift_range=0.2,
height_shift_range=0.2,
shear_range=0.2,
zoom_range=0.2,
horizontal_flip=True,
fill_mode='nearest',
)
test_datagen = ImageDataGenerator(rescale=1./255)

"""* Augments training images to prevent overfitting.
* Rescales validation/test images without augmentation

#Functions to load data
"""

def get_train_generator(sample_size, batch_size=32):
    return train_datagen.flow_from_directory(
        train_dir,
        target_size=(150, 150),
        batch_size=batch_size,
        class_mode='binary',
        shuffle=True,
        seed=42
    )
from tensorflow.keras.preprocessing.image import ImageDataGenerator
def get_validation_generator(batch_size=32):
    val_gen = test_datagen.flow_from_directory(
        validation_dir,
        target_size=(150, 150),
        batch_size=batch_size,
        class_mode='binary',
        shuffle=True, # Shuffle to get a random subset
        seed=42
    )
    # Ensure validation size is limited to 500
    val_gen.samples = min(val_gen.n, 500) # Limit to 500 samples if more exist
    return val_gen
def get_test_generator(batch_size=32):
    test_gen = test_datagen.flow_from_directory(
        test_dir,
        target_size=(150, 150),
        batch_size=batch_size,
        class_mode='binary',
        shuffle=False
    )
    # Ensure test size is limited to 500
    test_gen.samples = min(test_gen.n, 500) # Limit to 500 samples if more exist
    return test_gen

"""* Loads images from the dataset directory, resizes them to 150x150, and batches them for training.
* Loads validation images, shuffles them, and ensures a maximum of 500 samples.
* Loads test images without shuffling.

#Build a CNN from scratch
"""

def build_scratch_model():
    model = models.Sequential([
        layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)),
        layers.MaxPooling2D((2,2)),
        layers.Conv2D(64, (3,3), activation='relu'),
        layers.MaxPooling2D((2,2)),
        layers.Conv2D(128, (3,3), activation='relu'),
        layers.MaxPooling2D((2,2)),
        layers.Conv2D(128, (3,3), activation='relu'),
        layers.MaxPooling2D((2,2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),
        layers.Dense(1, activation='sigmoid')
    ])
    model.compile(loss='binary_crossentropy',
                  optimizer=keras.optimizers.Adam(1e-4),
                  metrics=['accuracy'])
    return model

"""* Defines a Convolutional Neural Network (CNN) with:
 - Four convolutional layers with increasing filters (32 → 64 → 128 → 128).
 - MaxPooling layers for downsampling.
 - Dropout (0.5) for regularization.
 - L2 regularization in the fully connected layer.
 - Sigmoid activation for binary classification

# Build a model using pretrained VGG16
"""

def build_pretrained_model():
    conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(150,150,3))
    conv_base.trainable = False

    model = models.Sequential([
        conv_base,
        layers.Flatten(),
        layers.Dense(256, activation='relu'),
        layers.Dense(1, activation='sigmoid')
    ])
    model.compile(loss='binary_crossentropy',
                 optimizer=keras.optimizers.Adam(1e-4),
                 metrics=['accuracy'])
    return model

"""* Uses VGG16 as a feature extractor.
* Freezes VGG16 layers.
* Adds custom layers for classification.

# Plot Training Curves
"""

def plot_learning_curves(history, title):
    """Plot the learning curves for accuracy and loss"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    # Accuracy
    ax1.plot(history.history['accuracy'])
    ax1.plot(history.history['val_accuracy'])
    ax1.set_title(f'Model Accuracy - {title}')
    ax1.set_ylabel('Accuracy')
    ax1.set_xlabel('Epoch')
    ax1.legend(['Train', 'Validation'], loc='lower right')

    # Loss
    ax2.plot(history.history['loss'])
    ax2.plot(history.history['val_loss'])
    ax2.set_title(f'Model Loss - {title}')
    ax2.set_ylabel('Loss')
    ax2.set_xlabel('Epoch')
    ax2.legend(['Train', 'Validation'], loc='upper right')

    plt.tight_layout()
    plt.show()

"""* Plots accuracy and loss curves for model performance visualization.

# Set up callbacks for training
"""

early_stopping = keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

"""Early Stopping: Stops training if validation loss doesn’t improve for 5 epochs.

# Model checkpoint
"""

checkpoint_callback = keras.callbacks.ModelCheckpoint(
    filepath='best_model_{epoch:02d}.h5',
    save_best_only=True,
    monitor='val_accuracy',
    mode='max'
)

"""Model Checkpoint:Saves the best model based on validation accuracy.

# Define expanded sample sizes to test
"""

sample_sizes = [1000, 1500, 500, 2000,]
batch_size = 32
epochs = 30

"""- Trains models on subsets of 500, 1000, 1500, 2000, and 2500 samples.
- Steps per Epoch: Calculated as sample_size // batch_size to avoid overfitting.

# Dictionary to store results
"""

results_scratch = {}
results_pretrained = {}

"""#Actual number of samples to confirm"""

val_gen = get_validation_generator()
test_gen = get_test_generator()

print(f"Validation images loaded: {val_gen.samples}")
print(f"Test images loaded: {test_gen.samples}")

"""# Train models from scratch"""

print("===== Training Models from Scratch =====")
for size in sample_sizes:
    print(f"\nTraining with {size} samples...")

    # Get data generators
    train_gen = get_train_generator(size, batch_size)
    val_gen = get_validation_generator()
    test_gen = get_test_generator()

    # Correctly calculate steps_per_epoch
    steps_per_epoch = size // batch_size

    # Build and train model
    model = build_scratch_model()
    history = model.fit(
        train_gen,
        steps_per_epoch=steps_per_epoch,
        epochs=epochs,
        validation_data=val_gen,
        callbacks=[early_stopping, checkpoint_callback],
        verbose=1
    )

    # Plot learning curves
    plot_learning_curves(history, f"Scratch Model - {size} samples")

    # Evaluate on test set
    test_loss, test_acc = model.evaluate(test_gen)
    results_scratch[size] = test_acc
    print(f"Scratch Model with {size} samples - Test Accuracy: {test_acc:.4f}")

    # Save the model for this sample size
    model.save(f'scratch_model_{size}_samples.h5')

"""**Process Recap:**
* The model is trained from scratch using different dataset sizes: 500, 1000, 1500, and 2000 samples.
* The model is trained for up to 30 epochs with EarlyStopping to prevent overfitting.
* The best model for each dataset size is saved and evaluated on the test dataset.

**Results Analysis:**
* Small datasets (500 samples):
 - Starts with an accuracy of ~54% in early epochs.
 - Over multiple epochs, accuracy fluctuates between 50%-60%.
 - Final test accuracy: ~66.2%.
 - The small dataset may not provide enough variability for the model to generalize well.
**1000 samples:**
 - Initial accuracy ~49-55%.
 - Gradual improvement to ~67-70%.
 - Final test accuracy: ~68.0%.
 - The model starts learning patterns but still has some generalization issues.
**1500 samples:**
 - Initial accuracy ~49%, but it quickly improves.
 - After 20+ epochs, validation accuracy crosses 70%.
 - Final test accuracy: ~71.2%.
 - The increased dataset size allows for better learning, reducing overfitting.
**2000 samples:**
 - Accuracy starts around 50-55%.
 - Validation accuracy consistently improves, stabilizing around 66-68%.
 - Final test accuracy: ~66.8%.
 - Though trained on more samples, it does not improve significantly beyond the 1500-sample model. This suggests that the model architecture may need tuning or more complex features for better performance.

# Train pretrained models
"""

print("\n===== Training Models with Pretrained VGG16 =====")
for size in sample_sizes:
    print(f"\nTraining with {size} samples...")

    # Get data generators
    train_gen = get_train_generator(size, batch_size)
    val_gen = get_validation_generator()
    test_gen = get_test_generator()

    # Correctly calculate steps_per_epoch
    steps_per_epoch = size // batch_size

    # Build and train model
    model = build_pretrained_model()
    history = model.fit(
        train_gen,
        steps_per_epoch=steps_per_epoch,
        epochs=epochs,
        validation_data=val_gen,
        callbacks=[early_stopping, checkpoint_callback],
        verbose=1
    )

    # Plot learning curves
    plot_learning_curves(history, f"Pretrained Model - {size} samples")

    # Evaluate on test set
    test_loss, test_acc = model.evaluate(test_gen)
    results_pretrained[size] = test_acc
    print(f"Pretrained Model with {size} samples - Test Accuracy: {test_acc:.4f}")

    # Save the model for this sample size
    model.save(f'pretrained_model_{size}_samples.h5')

"""**Process Recap:**
* Instead of training from scratch, a pretrained VGG16 model is used with frozen convolutional layers.
* Only the final dense layers are trained.
* The same dataset sizes (500, 1000, 1500, 2000 samples) are used.
* Training is faster and generalizes better.
**Results Analysis:*
- 500 samples:
 - Validation accuracy reaches 85-88% within the first few epochs.
 - Final test accuracy: 88.6%.
 - Even with fewer samples, VGG16 effectively extracts useful features.
- 1000 samples:
 - Starts at ~60% accuracy but improves quickly.
 - Validation accuracy peaks at ~87-90%.
 - Final test accuracy: ~87.9%.
 - The improvement compared to the scratch model is significant (~20% better).
-1500 samples:
 - Initial accuracy ~68%, then stabilizes around 88-90%.
 - Final test accuracy: ~88.5%.
 - Consistent performance gain.
- 2000 samples (not fully shown in your extract):
 - Likely follows the same pattern, achieving ~88-90% accuracy

# Create a DataFrame to summarize results
"""

df = pd.DataFrame({
    'Sample Size': sample_sizes,
    'Scratch Model': [results_scratch[size] for size in sample_sizes],
    'Pretrained VGG16': [results_pretrained[size] for size in sample_sizes]
})

# Compute improvement percentage
df['Improvement (%)'] = ((df['Pretrained VGG16'] - df['Scratch Model']) / df['Scratch Model']) * 100

# Sort by Sample Size in ascending order
df = df.sort_values(by='Sample Size')

# Print formatted summary table
print("\n=== Performance Summary ===")
print(df.round(4))

"""#Optimal Results"""

optimal_scratch = df.loc[df['Scratch Model'].idxmax()]
optimal_pretrained = df.loc[df['Pretrained VGG16'].idxmax()]

print("\n=== Optimal Results ===")
print(f"From Scratch: {optimal_scratch['Sample Size']} samples (Acc: {optimal_scratch['Scratch Model']:.2%})")
print(f"Pretrained: {optimal_pretrained['Sample Size']} samples (Acc: {optimal_pretrained['Pretrained VGG16']:.2%})")

"""#Model Comparison by Sample Size"""

# Sort the DataFrame by 'Sample Size'
df = df.sort_values(by='Sample Size')

plt.figure(figsize=(10, 6))
x = np.arange(len(df['Sample Size']))
width = 0.35

plt.bar(x - width/2, df['Scratch Model'], width, label='Scratch Model')
plt.bar(x + width/2, df['Pretrained VGG16'], width, label='Pretrained VGG16')

plt.xticks(x, df['Sample Size'])  # Ensure x-axis labels are correct
plt.xlabel('Training Sample Size')
plt.ylabel('Test Accuracy')
plt.title('Model Comparison by Sample Size')
plt.legend()
plt.grid(True)
plt.show()

"""# Function to plot training accuracy and loss"""

import matplotlib.pyplot as plt

# Function to plot training accuracy and loss
def plot_training_curves(history, model_name="Model"):
    """Plots training accuracy and loss curves."""
    epochs = range(1, len(history.history['accuracy']) + 1)

    plt.figure(figsize=(12, 5))

    # Accuracy Plot
    plt.subplot(1, 2, 1)
    plt.plot(epochs, history.history['accuracy'], 'o-', label='Training Accuracy')
    plt.plot(epochs, history.history['val_accuracy'], 'o-', label='Validation Accuracy')
    plt.title(f"{model_name} - Accuracy")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.grid()

    # Loss Plot
    plt.subplot(1, 2, 2)
    plt.plot(epochs, history.history['loss'], 'o-', label='Training Loss')
    plt.plot(epochs, history.history['val_loss'], 'o-', label='Validation Loss')
    plt.title(f"{model_name} - Loss")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid()

    # Save the plot
    plt.tight_layout()
    plt.savefig(f"{model_name.lower().replace(' ', '_')}_training_curves.png")
    plt.show()

# Pass the 'history' object obtained from training the pretrained model
# Assuming the last training loop was for the pretrained model, 'history' should contain its training history
plot_training_curves(history, "Pretrained VGG16")

import matplotlib.pyplot as plt

# Function to plot training accuracy and loss
def plot_training_curves(history, model_name="Model"):
    """Plots training accuracy and loss curves."""
    epochs = range(1, len(history.history['accuracy']) + 1)

    plt.figure(figsize=(12, 5))

    # Accuracy Plot
    plt.subplot(1, 2, 1)
    plt.plot(epochs, history.history['accuracy'], 'o-', label='Training Accuracy')
    plt.plot(epochs, history.history['val_accuracy'], 'o-', label='Validation Accuracy')
    plt.title(f"{model_name} - Accuracy")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.grid()

    # Loss Plot
    plt.subplot(1, 2, 2)
    plt.plot(epochs, history.history['loss'], 'o-', label='Training Loss')
    plt.plot(epochs, history.history['val_loss'], 'o-', label='Validation Loss')
    plt.title(f"{model_name} - Loss")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid()

    # Save the plot
    plt.tight_layout()
    plt.savefig(f"{model_name.lower().replace(' ', '_')}_training_curves.png")
    plt.show()

# Assuming 'history' from Global Variables is for the scratch model
plot_training_curves(history, "CNN from Scratch")